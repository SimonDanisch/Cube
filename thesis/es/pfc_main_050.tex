%
% VISUALIZACIÓN DE NUBES DE PUNTOS
%
\chapter{
	Visualización de nubes de puntos en GPU
	\label{nombre_referencia_al_capitulo_050}
}

Este capítulo se presenta como un resumen a los diferentes métodos de render basados en GPU. La sección \ref{s_rasterizacion} empieza mostrando los diferentes algoritmos que existen para la rasterización de splats. La sección \ref{s_blending} se centra en el blending de los diferentes splats, rasterizados en el paso previo, teniendo en cuenta la implicación que toman diferentes aspectos como la iluminación.


%
% SECCION - Técnicas de rasterización
%
\section[Primitivas en nubes de puntos]{
	Primitivas en nubes de puntos
	\label{s_primitivas}
}

\subsection{Surfel}

Los puntos de una nube son el resultado del muestreo de la superficie que representan, cada punto corresponde a un elemento de superficie describiendo la superficie de una pequeña vecindad. El \textit{surfel} (ver Figura \ref{figure_surfel}) es una primitiva diseñada para el renderizado de puntos y podría ser descrita únicamente mediante su posición y color o añadiendo otros parámetros como pueden ser la normal, radio, etc. 

\begin{figure}
	\centering
	\includegraphics[width=7 cm]{../figures/surfel.eps}
	\caption{Representación de un \textit{surfel} de una esfera.}
	\label{figure_surfel}
\end{figure}

\subsection{Splat}

Se podría ver la necesidad de trabajar con \textit{splats} que tuvieran asociado una máscara que modifique el canal \textit{alpha} (ver Figura \ref{figure_splat}), normalmente debido a que la muestra suele tener más relevancia en el centro del \textit{splat} debido a que el punto es la representación de un elemento diferencial de superficie. Este tipo de primitiva es conocida como \textit{splat}.

\begin{figure}
	\centering
	\includegraphics[width= 7 cm]{../figures/splat.eps}
	\caption{Construcción de un \textit{splat}.}
	\label{figure_splat}
\end{figure}


%
% SECCION - Técnicas de rasterización
%
\section[Técnicas de rasterización]{
	Técnicas de rasterización
	\label{s_rasterizacion}
}

El primer paso en el render de nubes de puntos, es determinar según la proyección de los \textit{surfels} que píxeles en el \textit{framebuffer} serán cubiertos por ellos. Puesto que no existe soporte el las librerías actuales para representar este tipo de primitivas, los \textit{surfels} tendrán que ser representados por otras como puntos o triángulos. De entre esas dos opciones, se  considera como más eficiente los puntos por delante de los triángulos, ya que por cada \textit{surfel} únicamente hay que almacenar la posición de un vértice en lugar de tres. Finalmente para renderizar la nube, habrá que dibujar el \textit{VBO}, pasándole el tipo \textit{GL\_POINTS}
\\
\lstset{language=C, breaklines=true, basicstyle=\footnotesize}
\begin{lstlisting}[frame=single]
glDrawArrays(GL_POINTS, &surfels[0], surfels.size());
\end{lstlisting}

\subsection[Sized-Fixed Points] {Sized-Fixed Points \label{ss_sized}}

De una forma un poco primitiva y puesto que un punto a priori puede no tener un tamaño definido, a la hora de mostrar una representación de la nube, se podría dibujar cada coordenada del conjunto de puntos, con un cuadrado de un tamaño definido. Para ello basta con activar en el programa.
\\
\lstset{language=C, breaklines=true, basicstyle=\footnotesize}
\begin{lstlisting}[frame=single]
glEnable(GL_PROGRAM_POINT_SIZE);
\end{lstlisting}

Mientras que en el \textit{vertex shader}, se define con \textit{gl\_PointSize} el tamaño en píxeles que se pretende que ocupe el punto al ser rasterizado. Este cuadrado tendrá como centro las coordenadas del punto \textbf{p}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/fixed-sized.png}
	\caption{Todos los puntos de la nube son renderizados con un cuadrado del mismo tamaño para toda la nube.}
\end{figure}

Como resultado, se obtiene una representación en la que cada punto de la nube se renderizará con el tamaño descrito.

Ver Apéndice Shaders \ref{sized-fixed}
\subsection[Image-aligned Squares] {Image-aligned Squares \label{ss_image-aligned_squares}}

En el anterior paso, el tamaño de la representación que cada punto tomaba en el \textit{framebuffer} era de alguna forma independiente a la cámara y al modelo.

De esta manera, si en lugar de entender el tamaño del punto como los píxeles que va a ocupar en la imagen final, se interpreta como una medida dentro del modelo, que podría ser el radio \textbf{r} del splat, al representarse, estos puntos tendrían que ser coherentes con su proyección.

Así que el tamaño en el \textit{screen-space} del punto OpenGL proyectado, tiene que ser ajustado en el \textit{vertex shader}. El tamaño \cite{perspcorrect} se aproxima haciendo el escorzo \textit{(perspective foreshort)} del radio $ r $ del \textit{surfel} usando el \textit{depth value} del centro \textbf{p} que ha de estar en coordenadas de la cámara.

\begin{eqnarray}
gl\_PointSize = 2r \cdot \dfrac{n}{p_{z}} \cdot \dfrac{h}{t - b'}
\end{eqnarray}

Donde $ n $, $ t $ y $ b $ son los parámetros \textit{near/top/bottom} del \textit{view frustum} y $ h $ denota la altura (en píxeles) del \textit{viewport}. En esta fórmula el término $ n/z $ corresponde a la proyección en el plano \textit{near}, mientras que $ h/(t- b) $ escala el resultado desde el plano \textit{near} a coordenadas de la imagen.

Todos los píxeles, generados de un punto, tienen el mismo \textit{depth value} lo que imposibilita hacer uso de las técnicas blending en zonas que visualmente se estén superponiendo. (ver sección \ref{s_blending})

Ver Apéndice Shaders \ref{image-aligned}

\subsection[Affinely Projected Point Sprites] {Affinely Projected Point Sprites \label{ss_affinely}}

Una mejor aproximación para representar superficies afines a nubes de puntos, es con discos orientados según un vector normal \textbf{n}, este método fue presentado en \textit{Bosch and Kobbelt}~\cite{affinely} .

Para ello se tendría que ajustar el tamaño del punto en el \textit{vertex shader} de la misma forma que en el paso anterior \ref{ss_image-aligned_squares}. Pero añadiendo a mayores en el \textit{fragment shader} alguna manera de determinar si este se corresponde con un fragmento de la proyección de un disco orientado con centro en la posición \textbf{p}, de radio \textbf{r} y con la normal \textbf{n}. 

Para cada uno de los píxeles $ (x, y)  \in  [-r \;,\; r] $, se puede calcular un valor de desplazamiento en profundidad $ \delta z $ como una función lineal dependiente del vector en coordenadas de la cámara \textbf{n} = $(n_{x}, n_{y}, n_{z})^{T}$ :

\begin{equation}
	\delta z = - \dfrac{n_{x}}{n_{z}} \cdot x - \dfrac{n_{y}}{n_{z}} \cdot y
	\label{delta_z}
\end{equation}



Este desplazamiento es usado para calcular la distancia 3D desde el centro del punto \textbf{p}, de manera que el fragmento $ (x,y) $ corresponde al punto si $ ||(x,y,\delta z)|| \leq r $. Los que no cumplan esta condición pueden ser fácilmente descartados usando el comando \textit{discard}. 

La variable vector \textit{gl\_PointCoord} del \textit{fragment shader} contiene las coordenadas bidimensionales que indican donde dentro de un punto OpenGL el fragmento está situado, con valores desde el $ 0.0 $ al $ 1.0 $ estos han de desplazarse a un rango $ [-0.5 \;,\; 0.5] $ para usarse en el cálculo de $ \delta z $.

Una posible implementación de este algoritmo en \textit{GLSL} sería:
\\
\begin{lstlisting}[frame=single]
vec3 test;

test.x = gl_PointCoord.x - 0.5;
test.y = gl_PointCoord.y - 0.5;
test.z = -(normal.x/normal.z) * test.x - (normal.y/normal.z) * test.y;

if (length(test) > 0.5)
    discard;
\end{lstlisting}

Uno de los inconvenientes usando Image-aligned Squares \ref{ss_image-aligned_squares} era que el valor de profundidad era constante por \textit{surfel}. Pero $ \delta z $ puede ser usado también para corregir esto. Partiendo de un valor de profundidad en espacio de la cámara $ z' = p_{z} + \delta z $ y el frustum el valor de profundidad del $ zbuffer(x,y) $ puede ser modificado

\begin{equation}
zbuffer(x,y) = \dfrac{1}{z'} \cdot \dfrac{fn}{f - n} + \dfrac{f}{f - n}
\label{z_buffer}
\end{equation} 

En comparación con la propuesta de \textit{Image-aligned Squares}, este método ofrece una mejor aproximación, especialmente en los contornos del objeto. Sin embargo el valor de profundidad $ \delta z $ es solo una aproximación, ya que se asume una proyección paralela (\ref{delta_z}), causando errores que provocan que los discos se vuelvan demasiado finos cuando estos son mirados bajo ángulos planos, lo que puede resultar en huecos en la imagen renderizada (ver Figura \ref{figure_affinely_holes}). 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/affinely-holes.png}
	\caption{Dragon, modelo de 845,281 puntos renderizado mediante Affinely Projected Point Sprites. Este método causa huecos en ángulos extremos.}
	\label{figure_affinely_holes}
\end{figure}

Ver Apéndice Shaders \ref{affinely}

\subsection[Perspective Correct Rasterization]{Perspective Correct Rasterization \label{ss_perspective_correct}}

El método de \textit{Affinely Projected Point Sprites} (\ref{ss_affinely}) causaba errores debido a la proyección paralela que en esta se asumía. Puesto que la proyección puede o no ser paralela, se tendrá que usar una aproximación mas precisa. Esta técnica fue la propuesta por \textit{Bosch et al.}~\cite{perspective} . 

La idea principal de este procedimiento parte en determinar el punto 3D correspondiente al píxel 2D mediante un \textit{ray casting} local.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/perspective-correct.eps}
	\caption{Proyección y Ray Casting.}
	\label{figure_perspective_correct}
\end{figure}

Fijándose en el \textit{pipeline} de transformaciones de OpenGL el primer paso para calcular \textbf{q} es invertir la transformación \textit{window-to-viewport}, de esta manera se mapeará el píxel \textit{(x,y)} a un punto 3D {$ \mathbf{q_{n}} $} en el plano \textit{near}. 

\begin{eqnarray}
q_{n} = 
\begin{pmatrix} 
	x \cdot \dfrac{r-l}{w} - \dfrac{r-l}{2} \\ 
	y \cdot \dfrac{t-b}{h} - \dfrac{t-b}{2} \\ 
	-n 
\end{pmatrix}
\end{eqnarray} 

Donde \textit{x/y} son las coordenadas relativas a la ventana \textit{gl\_FragCoord.xy}, \textit{b/t/l/r} son los parámetros \textit{bottom/top/left/right} del \textit{viewing frustum}, y \textit{w/h} denotan la \textit{altura/anchura} del \textit{viewport} (ver Figura \ref{figure_perspective_correct}).

Se proyectará un rayo desde el origen (el ojo de la cámara) a través de {$ \mathbf{q_{n}} $} y la intersección con el plano del \textit{surfel} \textbf{p} dará como resultado el punto \textbf{q}. En caso de que $ q_{n} \cdot n = 0 $ se determinará que el ángulo del \textit{splat} es perpendicular con respecto de la cámara así que se podrá descartar. 

\begin{equation}
q = q_{n} * \dfrac{p \cdot n}{q_{n} \cdot n}
\label{q}
\end{equation} 

Finalmente el píxel será descartado si $ \| q - p \| > r $. Por otro lado el valor de profundidad del \textit{z-buffer} puede ser ajustado insertando $ q_{z} $ como valor de $ z' $ en la ecuación (\ref{z_buffer}). 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/perspective-correct-flat.png}
	\caption{Suzanne, modelo de 7,958 puntos renderizado mediante Perspective Correct Rasterization.}
	\label{figure_perspective_correct_render}
\end{figure}

Ver Apéndice Shaders \ref{perspective}

%
% SECCION - Técnicas de Blending
%
\section[Técnicas de Blending]{
	Técnicas de Blending
	\label{s_blending}
}

Luego de determinar los píxeles que son cubiertos por la proyección de los \textit{surfels} en el anterior paso de rasterización, el siguiente implica iluminación y en general el acabado estético de la superficie \textit{shading}.

% Subsección
\subsection[Flat Shading] {Flat Shading \label{ss_flat}}

Puesto que cada punto tiene asociado un vector normal \textbf{n}, para unas ciertas propiedades de material y reflectancia una iluminación local puede ser evaluada por \textit{surfel}. Como resultado se obtendrá una constante de color $ \mathbf{c_{i}} $ para cada \textit{surfel}, similar al \textit{flat shading} en modelos poligonales. Ya que los puntos tienden a intersecar unos con otros, las representaciones mediante \textit{flat shading} conllevan a discontinuidades en el color.

\subsection[Gouraud Shading] {Gouraud Shading \label{ss_gouraud}}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/comparacion_blending.png}
	\caption{Lucy, modelo de 397,664 puntos. Renderizado mediante \textit{Flat Shading} (\textbf{izquierda}), \textit{Gouraud Shading} (\textbf{centro}), y \textit{Phong Shading} (\textbf{derecha}).}
	\label{figure_blending_comparative}
\end{figure}

Para conseguir un render mas fino, las discontinuidades en el color provocadas por el \textit{shading} pueden ser emborronadas mediante \textit{blending} de los valores de color $ \mathbf{c_{i}} $ de los puntos que se están superponiendo. Puesto que los cálculos de iluminación todavía son computados por \textit{surfel}, este tipo de \textit{blending} del color, conceptualmente corresponde al \textit{Gouraud shading} de modelos poligonales.

Para implementar este tipo de \textit{blending} cada punto de la nube irá asociado con una función, de modo que para cada \textit{surfel} sus fragmentos irán teniendo menor peso $ \mathbf{r}$ cuanto mas alejados de su centro estén dando como lugar a un \textit{splat}, teniendo así menor impacto en la mezcla final de color $ \mathbf{c(x, y)} $. De manera que en la reconstrucción final del \textit{framebuffer}, el color de la posición $ (x,y) $ vendrá dado por el peso promedio de todos los fragmentos $ \mathbf{i} $ que intenten cubrirlo.

\begin{equation}
\mathbf{c}(x,y) = \dfrac{\sum_{i}^{} r_{i}(x,y) \; c_{i}}{\sum_{i}^{} r_{i}(x,y)}
\end{equation}

Esta media puede ser implementada en OpenGL usando 2 pases de render. Primero, el \textit{alpha-blending} es configurado con funciones separadas de \textit{blend} para las componentes RGB y alpha \textit{EXT\_blend\_func\_separate} con
\\
\begin{lstlisting}[frame=single]
glEnable(GL_BLEND);
glBlendFuncSeparateEXT(GL_SRC_ALPHA, GL_ONE, GL_ONE, GL_ONE);
\end{lstlisting}

Así los \textit{splats} irán acumulando los valores de color y peso en
las componentes RGB y alpha del \textit{framebuffer} como $ (
\sum_{i}^{} r_{i} c_{i} \;,\; \sum_{i}^{} r_{i} ) $. Finalmente tendrá
que normalizarse la componente \textit{RGB} de cada píxel,
dividiéndose por su componente \textit{alpha}. Esto puede ser
conseguido enviando el resultado del anterior pase a un siguiente como
una textura, renderizando un rectángulo de tamaño de la ventana y
mapeando esta textura en el. Este truco conocido como
\textit{render-to-texture}, envía de nuevo cada píxel a través del
\textit{pipeline} de OpenGL, de modo que un simple \textit{fragment
  shader} puede realizar esta normalización, como la propuesta en \textit{Botsch and Kobbelt}~\cite{affinely} y en \textit{Guennebaud and Paulin}~\cite{Efficient_screen}. Esta acumulación debe de ser realizada usando \textit{buffers} con precisión de punto flotante de 16 bits, con motivo de evitar saturaciones o artefactos.

Para restringir el \textit{blend} únicamente a la superposición de los \textit{splats} vecinos de la misma superficie. Es necesario un tercer pase, denominado de visibilidad. Para cada \textit{frame}, el pase de visibilidad primero renderiza la nube únicamente en el \textit{depth buffer}. Luego en el pase de \textit{blending} se renderiza la nube de nuevo, pero esta vez calculando iluminación y acumulando el valor resultante de color usando \textit{additive alpha-blending} como lo descrito anteriormente. En este pase no se debe de actualizar el \textit{z-buffer} calculado en el pase de visibilidad, pero sí se añade un desplazamiento $\epsilon$ (ver Figura \ref{figure_epsilondepth}) a todos los valores de profundidad~\cite{surface_splatting}, lo que provocará que todos los fragmentos dentro de una distancia $\epsilon$ sean mezclados. El pase final de normalización realiza la necesaria división por el componente \textit{alpha} como se describió anteriormente. El resultado de los tres pases de render es representado en la Figura \ref{figure_gouraud_pipeline}.

\begin{figure}
	\centering
	\includegraphics[width=10 cm]{../figures/epsilondepth.eps}
	\caption{Corrección del \textit{depth buffer} con un valor $\epsilon$.}
	\label{figure_epsilondepth}
\end{figure}

El \textit{pseudocode} sería el siguiente
\begin{lstlisting}[frame=single]
// visibility pass
glDepthMask(GL_TRUE);
glDisable(GL_BLEND);
bind_visibility_shaders();
glDrawArrays(GL_POINTS, &cloud[0], cloud.size());

// blending pass
glDepthMask(GL_FALSE);
glEnable(GL_BLEND);
bind_blending_shaders();
glDrawArrays(GL_POINTS, &cloud[0], cloud.size());

// normalization pass
glCopyTexSubImage2D(GL_TEXTURE_RECTANGLE_ARB, 0, 0, 0, 0, 0, w, h);
bind_normalization_shaders();
draw_rectangle();
\end{lstlisting}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/gouraud-pipeline.eps}
	\caption{Los tres pases, \textit{visibility}, \textit{blending}, y \textit{normalization}.}
	\label{figure_gouraud_pipeline}
\end{figure}

Ver Apéndice Shaders \ref{gouraud}

\subsection[Phong Shading] {Phong Shading \label{ss_phong}}

Cuando se comparan los resultado de las diferentes técnicas en la figura \ref{figure_blending_comparative}, \textit{Gouraud} realmente elimina las indeseadas discontinuidades de color provocadas en el \textit{Flat Shading}, pero también añade cierto emborrone a la imagen notablemente. Para renderizado de polígonos es bien conocido que \textit{Phong} es superior que \textit{Gouraud Shading}. En lugar de calcular la iluminación por cada vértice y linealmente interpolar el color resultante dentro de los triángulos, \textit{Phong} interpola las normales en los vértices, seguido de una iluminación por píxel basada en la resultante función definida a trozos.

Pero ya que en el caso de las nubes de puntos falta la relación de conectividad, no es posible la interpolación de las normales de los puntos vecinales, de modo que el campo de normales deberá ser construido de otra forma.

El método de \textit{Phong} de \textit{Botsch et al.}~\cite{perspective} asigna explícitamente un campo de normales lineal $ \mathbf{n_{i}}(x,y) $ para cada \textit{splat} en lugar de mantener su normal asociada constante. Durante la rasterización, la normal es evaluada para cada píxel basándose en sus parámetros locales \textit{(x, y)}, y el vector resultante $ \mathbf{n_{i}}(x,y)$ es usado para computar la iluminación. Ya que los valores de color resultantes siguen teniendo problemas de discontinuidad, estos son acumulados y mezclados usando el mismo sistema de 3 pases de \textit{Gouraud}. La único que queda por conocer entonces es saber como computar el campo de normales lineal $ \mathbf{n_{i}}(x,y) $. Cada normal será representada por un punto (x,y) en el plano tangente con distancia 1, similar a las coordenadas homogéneas (Figura \ref{figure_phong_normal}). 

\begin{figure}
	\centering
	\includegraphics[width=7 cm]{../figures/phong.eps}
	\caption{Los vectores $ n_{i} $ son representados como puntos homogéneos \textit{(x,y,1)} en un plano desplazado tangente.}
	\label{figure_phong_normal}
\end{figure}

\begin{equation}
n_{i} = \dfrac{(q + n) - p}{\|(q + n) - p\|}
\end{equation}

Donde \textbf{q} es el punto 3D del \textit{splat} calculado al igual que en el método de \textit{Perspective Correct Rasterization} (Ecuación \ref{q}), \textbf{n} es la normal del \textit{splat} y \textbf{p} es el centro de este en coordenadas de la cámara.

Ver Apéndice Shaders \ref{phong}

\subsection[Deferred Shading] {Deferred Shading \label{ss_deferred}}

Básicamente existen dos formas para generar interpolaciones suavizadas de vectores de normales. La primera es \textit{Phong} que como se comentó en el anterior apartado, utiliza funciones para calcular el campo de normales de un \textit{splat}. Una segunda opción pasaría por la denominada como \textit{deferred} en la que en lugar de mezclar en un misma fase color y normales, estas se calculan de forma separada.

Recientes generaciones de \textit{GPU} contienen todo lo necesario para implementar esta aproximación, que gracias al denominado \textit{multiple render targets} (\textit{ARB\_DRAW\_BUFFERS}) permite que en un simple pase de \textit{render} se puedan sacar diferentes resultados a diferentes \textit{buffers}, estas características habilitan la opción para poder implementar una iluminación por píxel (\textit{deferred}) en este contexto, como se demostró en \textit{Botsch et al.}~\cite{deferred}

Primeramente en el pase de visibilidad (ver Figura \ref{figure_deferred_pipeline}, izquierda), se hará una modificación para poder sacar en un target a parte la posición 3D del fragmento, \textbf{q} (ver. Ecuación \ref{q}) ya que será necesario para el pase final. Luego, en el pase de acumulación y blend (ver Figura \ref{figure_deferred_pipeline}, centro) se usarán 2 targets adicionales para sacar la salida referente al color y de las normales de manera separada, usando el mismo proceso de acumulación que en \textit{Gouraud}. Finalmente estos 3 \textit{buffers} serán enviados de nuevo al \textit{pipeline} de OpenGL, usando el método de \textit{render to texture} comentado previamente, con motivo de su normalización.

Este último pase (ver Figura \ref{figure_deferred_pipeline}, derecha), corresponde a la fase de normalización de \textit{Gouraud} pero en el que a mayores se van a realizar los cálculos pertinentes para la iluminación. Por cada píxel, una media de la normal y el color pueden ser obtenidos, que junto a la posición obtenida en la primera fase de visualización puede ser fácilmente calcular el color final iluminado.

Es importante darse de cuenta de que en este método los cálculos derivados de la iluminación son computados una única vez por píxel, en contraste con las aproximaciones que no son \textit{deferred} que incorporan este proceso en el momento de la rasterización, consiguiendo el color final del píxel a partir de la mezcla de muchos píxeles de diferentes \textit{splats} iluminados previamente. Dependiendo del número de \textit{splats} superpuestos, esto puede llegar a multiplicar el numero de cálculos derivados de la iluminación.

Además a esto, \textit{deferred shading} añade también una clara separación entre la fase de rasterización y de iluminación.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/deferred_pipeline.eps}
	\caption{El renderizado mediante \textit{Deferred Shading} acumula atributos como color y normales, seguido por una normalización y un shading pass.}
	\label{figure_deferred_pipeline}
\end{figure}

Ver Apéndice Shaders \ref{deferred}

%
% FIN DEL CAPÍTULO
%
